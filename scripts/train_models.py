import sys
import os
import logging
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, r2_score

# =============================================================================
# ASCII HEADER - STYLE LIVRAISON PRO
# =============================================================================
#  _____                 _         ______ _             _   _____       _   _                 
# |  ___|               (_)        |  ___(_)           | | |  __ \     | | | |                
# | |__ __  ____ _ _ __  _ _ __    | |_   _ _ __   __ _| | | |  \/_   _| |_| |__  _   _ _ __  
# |  __|\ \/ / _` | '_ \| | '_ \   |  _| | | '_ \ / _` | | | | __| | | | __| '_ \| | | | '_ \ 
# | |___ >  < (_| | | | | | | | |  | |   | | | | | (_| | | | |_\ \ |_| | |_| | | | |_| | |_) |
# \____//_/\_\__,_|_| |_|_|_| |_|  \_|   |_|_| |_|\__,_|_|  \____/\__,_|\__|_| |_|\__,_| .__/ 
#                                                                                      | |    
#                                                                                      |_|    
# 
# Titre        : EntraÃ®nement des modÃ¨les (SupervisÃ©s & Non SupervisÃ©s)
# Auteur       : Adam Beloucif et Emilien MORICE
# Projet       : Examen Final Python Data Science
# Date         : 2026-02-26
# Description  : Script principal pour mon projet de fin d'annÃ©e. Ici on nettoie nos 
#                donnÃ©es et on entraÃ®ne quelques modÃ¨les vus en cours (RÃ©gression, 
#                Arbre, Random Forest).
# =============================================================================

# Forcer l'encodage de la console Windows en UTF-8 pour supporter les emojis et caractÃ¨res spÃ©ciaux
sys.stdout.reconfigure(encoding='utf-8')

# -----------------------------------------------------------------------------
# Configuration du Logging Professionnel
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# CrÃ©ation du rÃ©pertoire d'output s'il n'existe pas
os.makedirs("output", exist_ok=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 1 : CHARGEMENT ET NETTOYAGE DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_and_clean_data(filepath: str) -> pd.DataFrame:
    """
    Charge les donnÃ©es depuis un CSV et nettoie les anomalies.
    POURQUOI : Comme vu en TD, il y a souvent des valeurs bizarres dans les vrais datasets 
    (loyers Ã  1$). Les retirer aide nos modÃ¨les Ã  mieux apprendre la rÃ©alitÃ© du marchÃ©.
    """
    logger.info("ğŸ“¦ Je commence par charger le fichier CSV de base...")
    try:
        df = pd.read_csv(filepath, sep=';', encoding='cp1252')
        logger.info(f"âœ… DonnÃ©es chargÃ©es avec succÃ¨s : {df.shape[0]} lignes.")
    except Exception as e:
        logger.error(f"âŒ Erreur lors du chargement : {e}")
        sys.exit(1)

    # 1. SÃ©lection des features pertinentes (Feature Selection)
    # POURQUOI : On va faire simple et utiliser les variables numÃ©riques Ã©videntes 
    # (bathrooms, bedrooms, surface et latitude/longitude) pour pas s'embrouiller avec le texte.
    features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'price']
    df = df[features]
    
    # 2. Nettoyage des valeurs manquantes
    # POURQUOI : Les algos de sklearn de base gÃ¨rent pas les valeurs nulles (les NaN). 
    # Vu qu'on a beaucoup de donnÃ©es, jeter quelques lignes incomplÃ¨tes c'est plus sÃ»r qu'essayer de les deviner.
    initial_len = len(df)
    df = df.dropna()
    logger.info(f"ğŸ” Nettoyage NaN : {initial_len - len(df)} lignes supprimÃ©es.")

    # 3. Traitement des Outliers (Valeurs Aberrantes)
    # POURQUOI : Un loyer Ã  1$ Ã§a bousille completement nos moyennes. 
    # On garde juste les apparts normaux entre 300$ et 10,000$, et plus de 200 sqft.
    df = df[(df['price'] > 300) & (df['price'] < 10000)]
    df = df[(df['square_feet'] > 200) & (df['square_feet'] < 10000)]
    
    logger.info(f"ğŸ“Š DonnÃ©es finalisÃ©es aprÃ¨s filtrage des Outliers : {len(df)} annonces conservÃ©es.")
    
    return df

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 2 : PRE-PROCESSING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def preprocess_data(df: pd.DataFrame):
    """
    SÃ©pare les donnÃ©es entre X (les features) et y (ce qu'on veut prÃ©dire, le prix).
    POURQUOI : Les mÃ¨tres carrÃ©s (milliers) et les latitudes (dizaines) ont des Ã©chelles trÃ¨s diffÃ©rentes !
    Le StandardScaler met tout Ã  plat pour que les modÃ¨les apprennent proportionnellement sans favoriser la surface Ã  tort.
    """
    logger.info("âš™ï¸ DÃ©marrage du Pre-Processing : on sÃ©pare train/test et on applique un petit StandardScaler...")
    
    X = df.drop('price', axis=1)
    y = df['price']
    
    # Validation du split classique 80/20 pour garder assez de donnÃ©es d'apprentissage
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Sauvegarde du scaler pour pouvoir transformer les inputs de l'API plus tard
    joblib.dump(scaler, 'scaler.pkl')
    logger.info("âœ… Scaler sauvegardÃ© sous 'scaler.pkl'")
    
    # CrÃ©ation d'un dataset "Features" global pour l'appentissage Non-SupervisÃ©
    X_scaled_full = scaler.fit_transform(X)
    
    return X_train_scaled, X_test_scaled, y_train, y_test, X, X_scaled_full

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 3 : MODÃˆLES SUPERVISÃ‰S (PRÃ‰DICTION DU PRIX)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_supervised_models(X_train, X_test, y_train, y_test, feature_names):
    """
    EntraÃ®ne trois modÃ¨les supervisÃ©s et on garde le gagnant.
    POURQUOI : C'est la dÃ©marche super classique du cours : on compare d'abord un modÃ¨le basique 
    (RÃ©gression LinÃ©aire) face Ã  des modÃ¨les plus lourds comme Random Forest pour voir l'impact de la complexitÃ©.
    """
    logger.info("ğŸš€ C'est parti pour l'entraÃ®nement de nos modÃ¨les de Machine Learning...")
    
    models = {
        "RÃ©gression LinÃ©aire": LinearRegression(),
        "Arbre de DÃ©cision": DecisionTreeRegressor(random_state=42),
        "ForÃªt AlÃ©atoire (Random Forest)": RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    best_model = None
    best_r2 = -np.inf
    best_model_name = ""
    
    for name, model in models.items():
        logger.info(f"â³ EntraÃ®nement du modÃ¨le : {name}...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # MÃ‰TRIQUES
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        logger.info(f"   ğŸ“ˆ {name} -> RMSE: {rmse:.2f} | RÂ²: {r2:.4f}")
        
        if r2 > best_r2:
            best_r2 = r2
            best_model = model
            best_model_name = name
            
    logger.info(f"ğŸ† Le meilleur modÃ¨le supervisÃ© est '{best_model_name}' avec RÂ²={best_r2:.4f}")
    
    # Feature Importances (exclusif aux modÃ¨les type Random Forest/Decision Tree)
    # POURQUOI : Mon prof m'a dit qu'il faut toujours pouvoir expliquer son modÃ¨le. 
    # Afficher l'importance des variables montre ce qui compte vraiment pour bien prÃ©dire un loyer de faÃ§on concrÃ¨te.
    if hasattr(best_model, 'feature_importances_'):
        importances = best_model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        plt.figure(figsize=(10, 6))
        sns.barplot(x=importances[indices], y=[feature_names[i] for i in indices], palette='viridis')
        plt.title('Importance des Features (Analyse Business)', fontsize=14)
        plt.xlabel('Importance Relative')
        plt.ylabel('Variables explicatives')
        plt.tight_layout()
        plt.savefig('output/feature_importance.png')
        plt.close()
        logger.info("ğŸ“¸ Graphique 'feature_importance.png' gÃ©nÃ©rÃ© dans output/.")
        
    # Sauvegarde du meilleur modÃ¨le
    joblib.dump(best_model, 'model.pkl')
    logger.info("ğŸ“ ModÃ¨le final sauvegardÃ© sous 'model.pkl'")
    
    return best_model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 4 : MODÃˆLE NON SUPERVISÃ‰ (CLUSTERING GÃ‰OGRAPHIQUE / IMMOBILIER)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_unsupervised_model(X_scaled_full, X_original):
    """
    Petit modÃ¨le K-Means pour voir des tendances dans le marchÃ©.
    POURQUOI : Histoire de rajouter une touche non-supervisÃ©e au projet ! Ã‡a nous permet de 
    segmenter le marchÃ© de faÃ§on automatique, peut-Ãªtre qu'il regroupe de lui-mÃªme les petits studios vs les villas.
    """
    logger.info("ğŸ§  Et pour finir, on lance l'entraÃ®nement d'un petit clustering K-Means non supervisÃ©...")
    
    # HypothÃ¨se mÃ©tier : 4 types d'appartements principaux
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled_full)
    
    X_original['Cluster'] = clusters
    
    # Analyse visuelle des clusters selon la Surface et le nombre de Chambres
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=X_original, x='square_feet', y='bedrooms', hue='Cluster', palette='Set1', alpha=0.6)
    plt.title('Clustering K-Means : Segmentation des biens', fontsize=14)
    plt.xlabel('Surface (sq ft)')
    plt.ylabel('Nombre de Chambres')
    plt.tight_layout()
    plt.savefig('output/clustering_analysis.png')
    plt.close()
    logger.info("ğŸ“¸ Graphique 'clustering_analysis.png' gÃ©nÃ©rÃ© dans output/.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXÃ‰CUTION PRINCIPALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    filepath = "apartments_for_rent_classified_10K.csv"
    
    # 1. Chargement et nettoyage
    df_clean = load_and_clean_data(filepath)
    
    # 2. PrÃ©-traitement
    X_train, X_test, y_train, y_test, X, X_scaled_full = preprocess_data(df_clean)
    
    # 3. ModÃ©lisation SupervisÃ©e
    best_model = train_supervised_models(X_train, X_test, y_train, y_test, X.columns)
    
    # 4. ModÃ©lisation Non SupervisÃ©e
    train_unsupervised_model(X_scaled_full, X)
    
    logger.info("ğŸ‰ Fin de l'entraÃ®nement avec succÃ¨s. Tous les livrables sont gÃ©nÃ©rÃ©s !")
