import sys
import os
import logging
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, r2_score

# =============================================================================
# ASCII HEADER - STYLE LIVRAISON PRO
# =============================================================================
#  _____                 _         ______ _             _   _____       _   _                 
# |  ___|               (_)        |  ___(_)           | | |  __ \     | | | |                
# | |__ __  ____ _ _ __  _ _ __    | |_   _ _ __   __ _| | | |  \/_   _| |_| |__  _   _ _ __  
# |  __|\ \/ / _` | '_ \| | '_ \   |  _| | | '_ \ / _` | | | | __| | | | __| '_ \| | | | '_ \ 
# | |___ >  < (_| | | | | | | | |  | |   | | | | | (_| | | | |_\ \ |_| | |_| | | | |_| | |_) |
# \____//_/\_\__,_|_| |_|_|_| |_|  \_|   |_|_| |_|\__,_|_|  \____/\__,_|\__|_| |_|\__,_| .__/ 
#                                                                                      | |    
#                                                                                      |_|    
# 
# Titre        : EntraÃ®nement des modÃ¨les (SupervisÃ©s & Non SupervisÃ©s)
# Auteur       : Adam Beloucif et Emilien MORICE
# Projet       : Examen Final Python Data Science
# Date         : 2026-02-26
# Description  : Nettoyage, Pre-processing, EntraÃ®nement de 3 modÃ¨les supervisÃ©s et 
#                1 cluster non supervisÃ©, Ã©valuation dÃ©taillÃ©e, et exports des poids.
# =============================================================================

# Forcer l'encodage de la console Windows en UTF-8 pour supporter les emojis et caractÃ¨res spÃ©ciaux
sys.stdout.reconfigure(encoding='utf-8')

# -----------------------------------------------------------------------------
# Configuration du Logging Professionnel
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# CrÃ©ation du rÃ©pertoire d'output s'il n'existe pas
os.makedirs("output", exist_ok=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 1 : CHARGEMENT ET NETTOYAGE DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_and_clean_data(filepath: str) -> pd.DataFrame:
    """
    Charge les donnÃ©es depuis un CSV et applique les rÃ¨gles de nettoyage.
    POURQUOI : Le jeu de donnÃ©es initial contient beaucoup de valeurs aberrantes 
    (prix irrÃ©alistes) et des valeurs manquantes qui fausseraient l'apprentissage 
    de nos modÃ¨les immobiliers.
    """
    logger.info("ğŸ“¦ DÃ©marrage du chargement des donnÃ©es brutes...")
    try:
        df = pd.read_csv(filepath, sep=';', encoding='cp1252')
        logger.info(f"âœ… DonnÃ©es chargÃ©es avec succÃ¨s : {df.shape[0]} lignes.")
    except Exception as e:
        logger.error(f"âŒ Erreur lors du chargement : {e}")
        sys.exit(1)

    # 1. SÃ©lection des features pertinentes (Feature Selection mÃ©tier)
    # POURQUOI : Simplifier le modÃ¨le en Ã©cartant les colonnes non structurÃ©es (textes)
    # et conserver celles directement liÃ©es au prix du logement selon l'intuition mÃ©tier.
    features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'price']
    df = df[features]
    
    # 2. Nettoyage des valeurs manquantes
    # POURQUOI : Les algorithmes comme la rÃ©gression linÃ©aire ou Random Forest 
    # ne supportent pas les NaN. La suppression est la mÃ©thode la plus sÃ»re si le volume est faible.
    initial_len = len(df)
    df = df.dropna()
    logger.info(f"ğŸ” Nettoyage NaN : {initial_len - len(df)} lignes supprimÃ©es.")

    # 3. Traitement des Outliers (Valeurs Aberrantes)
    # POURQUOI : Un loyer Ã  1$ ou 1M$ biaise la moyenne et les prÃ©dictions. 
    # On isole les biens "standards" (Ex: Loyer entre 300$ et 10,000$, surface > 200 sqft).
    df = df[(df['price'] > 300) & (df['price'] < 10000)]
    df = df[(df['square_feet'] > 200) & (df['square_feet'] < 10000)]
    
    logger.info(f"ğŸ“Š DonnÃ©es finalisÃ©es aprÃ¨s filtrage des Outliers : {len(df)} annonces conservÃ©es.")
    
    return df

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 2 : PRE-PROCESSING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def preprocess_data(df: pd.DataFrame):
    """
    SÃ©pare les donnÃ©es en variables explicatives (X) et cible (y), 
    puis applique une normalisation.
    POURQUOI : Les diffÃ©rentes variables n'ont pas la mÃªme Ã©chelle (prix vs latitude).
    La normalisation (StandardScaler) aide les modÃ¨les basÃ©s sur les distances et optimise 
    la convergence des algorithmes.
    """
    logger.info("âš™ï¸ DÃ©marrage du Pre-Processing (Split & Scaling)...")
    
    X = df.drop('price', axis=1)
    y = df['price']
    
    # Validation du split classique 80/20 pour garder assez de donnÃ©es d'apprentissage
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Sauvegarde du scaler pour pouvoir transformer les inputs de l'API plus tard
    joblib.dump(scaler, 'scaler.pkl')
    logger.info("âœ… Scaler sauvegardÃ© sous 'scaler.pkl'")
    
    # CrÃ©ation d'un dataset "Features" global pour l'appentissage Non-SupervisÃ©
    X_scaled_full = scaler.fit_transform(X)
    
    return X_train_scaled, X_test_scaled, y_train, y_test, X, X_scaled_full

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 3 : MODÃˆLES SUPERVISÃ‰S (PRÃ‰DICTION DU PRIX)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_supervised_models(X_train, X_test, y_train, y_test, feature_names):
    """
    EntraÃ®ne trois modÃ¨les supervisÃ©s diffÃ©rents et sÃ©lectionne le meilleur.
    POURQUOI : Permet de comparer un modÃ¨le linÃ©aire simple avec des modÃ¨les non linÃ©aires 
    arborescents qui captent mieux les complexitÃ©s mÃ©tier.
    """
    logger.info("ğŸš€ DÃ©marrage de l'entraÃ®nement des modÃ¨les supervisÃ©s...")
    
    models = {
        "RÃ©gression LinÃ©aire": LinearRegression(),
        "Arbre de DÃ©cision": DecisionTreeRegressor(random_state=42),
        "ForÃªt AlÃ©atoire (Random Forest)": RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    best_model = None
    best_r2 = -np.inf
    best_model_name = ""
    
    for name, model in models.items():
        logger.info(f"â³ EntraÃ®nement du modÃ¨le : {name}...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # MÃ‰TRIQUES
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        logger.info(f"   ğŸ“ˆ {name} -> RMSE: {rmse:.2f} | RÂ²: {r2:.4f}")
        
        if r2 > best_r2:
            best_r2 = r2
            best_model = model
            best_model_name = name
            
    logger.info(f"ğŸ† Le meilleur modÃ¨le supervisÃ© est '{best_model_name}' avec RÂ²={best_r2:.4f}")
    
    # Feature Importances (exclusif aux modÃ¨les ensemblistes comme Random Forest)
    # POURQUOI : Pour le reporting business, il faut expliquer quelles sont les variables 
    # qui font grimper le prix du loyer.
    if hasattr(best_model, 'feature_importances_'):
        importances = best_model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        plt.figure(figsize=(10, 6))
        sns.barplot(x=importances[indices], y=[feature_names[i] for i in indices], palette='viridis')
        plt.title('Importance des Features (Analyse Business)', fontsize=14)
        plt.xlabel('Importance Relative')
        plt.ylabel('Variables explicatives')
        plt.tight_layout()
        plt.savefig('output/feature_importance.png')
        plt.close()
        logger.info("ğŸ“¸ Graphique 'feature_importance.png' gÃ©nÃ©rÃ© dans output/.")
        
    # Sauvegarde du meilleur modÃ¨le
    joblib.dump(best_model, 'model.pkl')
    logger.info("ğŸ“ ModÃ¨le final sauvegardÃ© sous 'model.pkl'")
    
    return best_model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAVAIL 4 : MODÃˆLE NON SUPERVISÃ‰ (CLUSTERING GÃ‰OGRAPHIQUE / IMMOBILIER)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_unsupervised_model(X_scaled_full, X_original):
    """
    EntraÃ®ne un modÃ¨le K-Means pour grouper les annonces.
    POURQUOI : DÃ©couvrir des 'segments' de biens immobiliers sans indications prÃ©alables.
    Par exemple, segmenter en "Biens de luxe", "Biens familiaux", etc.
    """
    logger.info("ğŸ§  DÃ©marrage de l'entraÃ®nement du modÃ¨le Non-SupervisÃ© (K-Means)...")
    
    # HypothÃ¨se mÃ©tier : 4 types d'appartements principaux
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled_full)
    
    X_original['Cluster'] = clusters
    
    # Analyse visuelle des clusters selon la Surface et le nombre de Chambres
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=X_original, x='square_feet', y='bedrooms', hue='Cluster', palette='Set1', alpha=0.6)
    plt.title('Clustering K-Means : Segmentation des biens', fontsize=14)
    plt.xlabel('Surface (sq ft)')
    plt.ylabel('Nombre de Chambres')
    plt.tight_layout()
    plt.savefig('output/clustering_analysis.png')
    plt.close()
    logger.info("ğŸ“¸ Graphique 'clustering_analysis.png' gÃ©nÃ©rÃ© dans output/.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXÃ‰CUTION PRINCIPALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    filepath = "apartments_for_rent_classified_10K.csv"
    
    # 1. Chargement et nettoyage
    df_clean = load_and_clean_data(filepath)
    
    # 2. PrÃ©-traitement
    X_train, X_test, y_train, y_test, X, X_scaled_full = preprocess_data(df_clean)
    
    # 3. ModÃ©lisation SupervisÃ©e
    best_model = train_supervised_models(X_train, X_test, y_train, y_test, X.columns)
    
    # 4. ModÃ©lisation Non SupervisÃ©e
    train_unsupervised_model(X_scaled_full, X)
    
    logger.info("ğŸ‰ Fin de l'entraÃ®nement avec succÃ¨s. Tous les livrables sont gÃ©nÃ©rÃ©s !")
